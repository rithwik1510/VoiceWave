# Competitive Benchmark v1

## Scope

This benchmark compares VoiceWave against:

1. Wispr Flow
2. Raycast dictation workflow
3. Superwhisper
4. MacWhisper

## Why These Peers

1. They represent practical dictation workflows used by professionals.
2. They provide meaningful UX/performance/privacy contrasts.
3. They are strong parity references for launch positioning.

## Evaluation Dimensions

1. Local-only privacy posture
2. Setup friction and first-run clarity
3. Latency and responsiveness
4. Insertion reliability in productivity/coding apps
5. Model management transparency
6. Error handling and fallback UX
7. Permission guidance quality
8. Battery/thermal impact

## Scoring Rubric

1. Score each dimension 1-5.
2. Weighting:
   - reliability and latency: 30%
   - privacy and trust: 25%
   - setup/onboarding: 15%
   - insertion UX and fallbacks: 20%
   - performance efficiency (battery/thermal): 10%
3. Collect evidence with reproducible test notes.

## Parity Targets for VoiceWave v1

1. Must-win:
   - reliable push-to-talk and release-to-insert
   - low-latency local transcription
   - clear state feedback and fallback UX
2. Competitive baseline:
   - easy setup with transparent permissions
   - model selection and status clarity
3. Differentiator:
   - explicit local-core trust messaging with deterministic state model

## Data Collection Template

For each peer and each test app:

1. Setup time to first successful dictation
2. Success/failure of insertion attempt
3. Approximate latency observations
4. Permission blockers encountered
5. Recovery behavior on failure
6. Notes on reliability under app switching

## Supported App Matrix Under Test

1. Chrome
2. Edge
3. Safari (macOS)
4. Google Docs (browser)
5. Slack (desktop/web)
6. Notion (desktop/web)
7. VS Code
8. Cursor
9. Notepad (Windows)
10. Notes (macOS)
